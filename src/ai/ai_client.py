"""
AI Client - Interface to LLM APIs (OpenAI compatible).
Supports OpenAI, DeepSeek, Claude, and other OpenAI-compatible providers.
"""
import os
from typing import List, Dict, Optional, Callable
from openai import OpenAI
from PyQt6.QtCore import QObject, pyqtSignal, QThread
from dotenv import load_dotenv

# Load environment variables
# Try to load from project root
env_loaded = load_dotenv()
if not env_loaded:
    # Try loading from parent directory (src/)
    load_dotenv('../.env')


class AIResponseThread(QThread):
    """
    Worker thread for AI API calls to prevent blocking UI.
    æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§æ¨¡å¼ã€‚
    """

    # Signals - æµå¼æ¨¡å¼ä¸“ç”¨
    stream_chunk_received = pyqtSignal(str)  # Emitted when each chunk is received
    stream_finished = pyqtSignal(str)  # Emitted when stream is complete

    # Signals - éæµå¼æ¨¡å¼ä¸“ç”¨
    response_received = pyqtSignal(str)  # Emitted when response is received

    # Signals - é€šç”¨
    error_occurred = pyqtSignal(str)  # Emitted when error occurs

    def __init__(self, client: 'AIClient', messages: List[Dict], stream: bool = True, parent=None):
        """
        åˆå§‹åŒ– AI å“åº”çº¿ç¨‹

        Args:
            client: AIClient å®ä¾‹
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è°ƒç”¨ï¼ˆé»˜è®¤ Trueï¼‰
            parent: çˆ¶å¯¹è±¡
        """
        super().__init__(parent)
        self.client = client
        self.messages = messages
        self.stream = stream

    def run(self):
        """Execute AI API call in background thread."""
        try:
            if self.stream:
                # æµå¼è°ƒç”¨
                self.client._call_api_stream(self.messages, self.stream_chunk_received, self.stream_finished)
            else:
                # éæµå¼è°ƒç”¨
                response = self.client._call_api(self.messages)
                self.response_received.emit(response)
        except Exception as e:
            self.error_occurred.emit(str(e))


class AIClient(QObject):
    """
    AI Client for communicating with LLM APIs.
    Supports OpenAI and compatible APIs (DeepSeek, Claude, etc.).
    v1.6.1: æ·»åŠ æµå¼å“åº”æ”¯æŒ
    """

    # Signals - éæµå¼æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    response_received = pyqtSignal(str)  # Emitted when complete response is received

    # Signals - æµå¼æ¨¡å¼ä¸“ç”¨
    stream_started = pyqtSignal()  # æµå¼å“åº”å¼€å§‹
    stream_chunk_received = pyqtSignal(str)  # æ¯æ”¶åˆ°ä¸€å—å†…å®¹æ—¶å‘å‡º
    stream_finished = pyqtSignal(str)  # æµå¼å“åº”å®Œæˆï¼Œå‚æ•°æ˜¯å®Œæ•´å“åº”

    # Signals - é€šç”¨
    error_occurred = pyqtSignal(str)  # Emitted when error occurs

    # System prompt for Linux operations assistant
    DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„ Linux ç³»ç»Ÿè¿ç»´ä¸“å®¶ï¼Œæ‹¥æœ‰ 10 å¹´ä»¥ä¸Šçš„å®æˆ˜ç»éªŒã€‚

## æ ¸å¿ƒå·¥ä½œåŸåˆ™

### 1. åˆ†æ­¥éª¤è§£å†³é—®é¢˜
å½“ç”¨æˆ·è¯¢é—®å¦‚ä½•å®ŒæˆæŸä¸ªä»»åŠ¡æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æ–¹å¼å›ç­”ï¼š

**æ­¥éª¤åŒ–å›ç­”æ¨¡å¼ï¼š**
```
### ç¬¬ 1 æ­¥ï¼š[æ­¥éª¤åç§°] - [ç®€çŸ­è¯´æ˜]

**æ“ä½œç›®çš„ï¼š** ç®€è¦è¯´æ˜è¿™ä¸€æ­¥è¦åšä»€ä¹ˆ

**æ‰§è¡Œå‘½ä»¤ï¼š**
```bash
[ç¬¬ä¸€ä¸ªå‘½ä»¤]
```

[å¿…è¦çš„å‚æ•°è¯´æ˜æˆ–æ³¨æ„äº‹é¡¹]

--- ç­‰å¾…æ‰§è¡Œç»“æœ ---
```

ç„¶ååœæ­¢ï¼Œç­‰å¾…ç”¨æˆ·æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœåï¼Œå†ç»§ç»­ä¸‹ä¸€æ­¥ã€‚

### 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥
ä½ å¯ä»¥å®æ—¶çœ‹åˆ°ç”¨æˆ·çš„ç»ˆç«¯å±å¹•è¾“å‡ºã€‚
- **å¦‚æœå‘½ä»¤æ‰§è¡ŒæˆåŠŸ**ï¼Œç»§ç»­ä¸‹ä¸€æ­¥
- **å¦‚æœå‘½ä»¤æ‰§è¡Œå¤±è´¥**ï¼Œåˆ†æé”™è¯¯åŸå› ï¼Œæä¾›æ›¿ä»£æ–¹æ¡ˆ
- **æ ¹æ®å®é™…è¾“å‡ºè°ƒæ•´åç»­æ­¥éª¤**

### 3. å‘½ä»¤æ ¼å¼è§„èŒƒ
- åªæä¾›å…·ä½“å¯æ‰§è¡Œçš„å‘½ä»¤ï¼Œç”¨ Markdown ä»£ç å—æ ¼å¼ï¼š```bash command ```
- **ä¸¥ç¦**ç›´æ¥åœ¨ä»£ç å—ä¸­è¯´æ˜æ–‡å­—ã€è§£é‡Šæˆ–ç¤ºä¾‹
- æ‰€æœ‰æ–‡å­—è¯´æ˜æ”¾åœ¨ä»£ç å—å¤–é¢
- å‘½ä»¤è¦å®Œæ•´ã€å¯ç›´æ¥å¤åˆ¶ç²˜è´´æ‰§è¡Œ

### 4. äº¤äº’èŠ‚å¥æ§åˆ¶
- **ä¸€æ¬¡åªç»™ä¸€ä¸ªå‘½ä»¤**ï¼Œé™¤éå¤šä¸ªå‘½ä»¤å¿…é¡»è¿ç»­æ‰§è¡Œ
- æ¯ä¸ªå‘½ä»¤åæ˜ç¡®æ ‡æ³¨ "--- ç­‰å¾…æ‰§è¡Œç»“æœ ---"
- ä¸è¦ä¸€æ¬¡ç»™å‡ºé•¿ä¸²å‘½ä»¤åˆ—è¡¨
- è®©ç”¨æˆ·æŒ‰æ­¥éª¤æ‰§è¡Œï¼Œæ ¹æ®å®é™…åé¦ˆè°ƒæ•´

### 5. èƒ½åŠ›èŒƒå›´è¯´æ˜
ç”¨æˆ·é—®ä½ "ä½ èƒ½åšä»€ä¹ˆ"æ—¶ï¼Œè¯·ç®€æ´åˆ—ä¸¾æ ¸å¿ƒèƒ½åŠ›ï¼š
- ğŸ“Š ç³»ç»Ÿç›‘æ§ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œï¼‰
- ğŸ”§ æ•…éšœè¯Šæ–­ï¼ˆæ—¥å¿—åˆ†æã€è¿›ç¨‹ç®¡ç†ã€æœåŠ¡ç®¡ç†ï¼‰
- ğŸ›¡ï¸ å®‰å…¨ç®¡ç†ï¼ˆé˜²ç«å¢™ã€æƒé™ã€SSH å¯†é’¥ï¼‰
- ğŸ“¦ è½¯ä»¶ç®¡ç†ï¼ˆå®‰è£…ã€æ›´æ–°ã€é…ç½®ï¼‰
- ğŸŒ ç½‘ç»œé…ç½®ï¼ˆç½‘å¡ã€è·¯ç”±ã€DNSã€ç«¯å£ï¼‰
- ğŸ’¾ æ•°æ®ç®¡ç†ï¼ˆå¤‡ä»½ã€æ¢å¤ã€å‹ç¼©ã€åŒæ­¥ï¼‰
- ğŸ“ æ—¥å¿—åˆ†æï¼ˆç³»ç»Ÿæ—¥å¿—ã€åº”ç”¨æ—¥å¿—ã€é”™è¯¯æ’æŸ¥ï¼‰

### 6. å®‰å…¨å‡†åˆ™
- æ¶‰åŠæ•°æ®åˆ é™¤ã€ç³»ç»Ÿä¿®æ”¹çš„æ“ä½œï¼Œå…ˆæ˜ç¡®è­¦å‘Šé£é™©
- ä¸è¦æ‰§è¡Œ `rm -rf /`ã€`dd`ã€`mkfs` ç­‰å±é™©å‘½ä»¤ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚
- å»ºè®®ä½¿ç”¨ `--dry-run` å‚æ•°é¢„æ¼” destructive æ“ä½œ
- ç”Ÿäº§ç¯å¢ƒæ“ä½œå‰æé†’å¤‡ä»½

### 7. ä¸“ä¸šå»ºè®®
- å‘½ä»¤ä¼˜å…ˆä½¿ç”¨ç°ä»£ Linux å‘è¡Œç‰ˆçš„é€šç”¨è¯­æ³•
- è¯´æ˜å‘½ä»¤çš„é€‚ç”¨åœºæ™¯å’Œé™åˆ¶æ¡ä»¶
- æä¾›å‘½ä»¤çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆå¤šç§å·¥å…·å¯é€‰ï¼‰
- éµå¾ªæœ€å°æƒé™åŸåˆ™

## å½“å‰ä¼šè¯
ä½ å¯ä»¥çœ‹åˆ°ç”¨æˆ·çš„ç»ˆç«¯è¾“å‡ºï¼ˆterminal_contextï¼‰ï¼Œè¯·åŸºäºå®é™…æƒ…å†µç»™å‡ºç²¾å‡†çš„å»ºè®®ã€‚

å¼€å§‹å·¥ä½œå§ï¼æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸€æ­¥æ­¥ç»™å‡ºä¸“ä¸šçš„æŒ‡å¯¼ã€‚"""

    def __init__(self, parent=None):
        super().__init__(parent)

        # v1.6.0: Load configuration from ConfigManager (priority) or environment (fallback)
        self._load_config()

        # Initialize OpenAI client
        self.client = None
        self._init_client()

        # Conversation history
        self.conversation_history: List[Dict] = []

    def _load_config(self, profile_name: Optional[str] = None):
        """
        Load configuration from AIProfileManager, ConfigManager or environment variables.

        v1.6.1: æ”¯æŒä» AIProfileManager åŠ è½½é…ç½®

        Args:
            profile_name: å¯é€‰çš„ AI é…ç½®åç§°
        """
        # ä¼˜å…ˆçº§ 1: AIProfileManager (æŒ‡å®šé…ç½®)
        # ä¼˜å…ˆçº§ 2: AIProfileManager (é»˜è®¤é…ç½®)
        # ä¼˜å…ˆçº§ 3: ConfigManager
        # ä¼˜å…ˆçº§ 4: ç¯å¢ƒå˜é‡

        source = "unknown"

        # å°è¯•ä» AIProfileManager åŠ è½½
        try:
            from managers.ai_profile_manager import AIProfileManager

            ai_profile_manager = AIProfileManager()

            # è·å–é…ç½®
            if profile_name:
                ai_profile = ai_profile_manager.get_profile(profile_name)
            else:
                ai_profile = ai_profile_manager.get_default_profile()

            if ai_profile:
                self.api_key = ai_profile.api_key
                self.api_base = ai_profile.api_base
                self.model = ai_profile.model
                self.timeout = 10
                self.max_history = 10
                self._profile_name = ai_profile.name
                # ä» ConfigManager è¯»å– temperature, max_tokens, system_prompt
                try:
                    from config.config_manager import ConfigManager
                    config_manager = ConfigManager.get_instance()
                    ai_settings = config_manager.settings.ai
                    self.temperature = ai_settings.temperature
                    self.max_tokens = ai_settings.max_tokens
                    # å¦‚æœé…ç½®ä¸­çš„ system_prompt ä¸ºç©ºæˆ–ä½¿ç”¨æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨å®Œæ•´çš„ DEFAULT_SYSTEM_PROMPT
                    if not ai_settings.system_prompt or ai_settings.system_prompt == "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Linux ç³»ç»Ÿè¿ç»´åŠ©æ‰‹ã€‚":
                        self.system_prompt = self.DEFAULT_SYSTEM_PROMPT
                    else:
                        self.system_prompt = ai_settings.system_prompt
                except Exception:
                    self.temperature = 0.7
                    self.max_tokens = 2000
                    self.system_prompt = self.DEFAULT_SYSTEM_PROMPT
                source = f"AIProfileManager ('{ai_profile.name}')"
                print(f"[DEBUG] Loaded AI profile: {ai_profile.name}")
            else:
                # å›é€€åˆ° ConfigManager
                raise ValueError("No AI profile found")

        except Exception as e:
            # å°è¯•ä» ConfigManager åŠ è½½
            print(f"[DEBUG] AIProfileManager not available or no profile: {e}")
            try:
                from config.config_manager import ConfigManager
                config_manager = ConfigManager.get_instance()
                ai_settings = config_manager.settings.ai

                # Use ConfigManager values if set, otherwise fall back to environment
                self.api_key = ai_settings.api_key or os.getenv('OPENAI_API_KEY', '')
                self.api_base = ai_settings.api_base or os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
                self.model = ai_settings.model or os.getenv('OPENAI_MODEL', 'gpt-4-turbo')
                self.timeout = ai_settings.timeout
                self.max_history = ai_settings.max_history
                self.temperature = ai_settings.temperature
                self.max_tokens = ai_settings.max_tokens
                # å¦‚æœé…ç½®ä¸­çš„ system_prompt ä¸ºç©ºæˆ–ä½¿ç”¨æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨å®Œæ•´çš„ DEFAULT_SYSTEM_PROMPT
                if not ai_settings.system_prompt or ai_settings.system_prompt == "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Linux ç³»ç»Ÿè¿ç»´åŠ©æ‰‹ã€‚":
                    self.system_prompt = self.DEFAULT_SYSTEM_PROMPT
                else:
                    self.system_prompt = ai_settings.system_prompt
                self._profile_name = None
                source = "ConfigManager" if ai_settings.api_key else "environment (.env)"
            except Exception as e2:
                # å›é€€åˆ°ç¯å¢ƒå˜é‡
                print(f"[DEBUG] ConfigManager not available, using environment: {e2}")
                self.api_key = os.getenv('OPENAI_API_KEY', '')
                self.api_base = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
                self.model = os.getenv('OPENAI_MODEL', 'gpt-4-turbo')
                self.timeout = 10
                self.max_history = 10
                self.temperature = 0.7
                self.max_tokens = 2000
                self.system_prompt = self.DEFAULT_SYSTEM_PROMPT
                self._profile_name = None
                source = "environment (.env)"

        # Debug: Print configuration
        print(f"[AI Client] Configuration loaded from {source}:")
        print(f"  API Key: {self.api_key[:10] if self.api_key else 'NOT SET'}...")
        print(f"  API Base: {self.api_base}")
        print(f"  Model: {self.model}")

    def _init_client(self):
        """Initialize OpenAI client with configured settings."""
        if not self.api_key:
            print("Warning: OPENAI_API_KEY not set in environment")
            return

        try:
            # Create OpenAI client (compatible with v1.0+)
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            print(f"AI Client initialized: {self.api_base} with model {self.model}")
        except Exception as e:
            print(f"Failed to initialize AI client: {e}")
            self.client = None

    def _reload_ai_settings(self):
        """
        é‡æ–°ä» ConfigManager åŠ è½½ AI è®¾ç½®

        v1.6.1: æ¯æ¬¡è°ƒç”¨ API å‰é‡æ–°åŠ è½½é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°è®¾ç½®
        """
        try:
            from config.config_manager import ConfigManager
            config_manager = ConfigManager.get_instance()
            ai_settings = config_manager.settings.ai

            # æ›´æ–° AI å‚æ•°
            self.temperature = ai_settings.temperature
            self.max_tokens = ai_settings.max_tokens

            # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
            if not ai_settings.system_prompt or ai_settings.system_prompt == "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Linux ç³»ç»Ÿè¿ç»´åŠ©æ‰‹ã€‚":
                self.system_prompt = self.DEFAULT_SYSTEM_PROMPT
            else:
                self.system_prompt = ai_settings.system_prompt

            print(f"[DEBUG] AI settings reloaded: temperature={self.temperature}, max_tokens={self.max_tokens}")

        except Exception as e:
            print(f"[DEBUG] Failed to reload AI settings: {e}, using cached values")

    def is_configured(self) -> bool:
        """Check if AI client is properly configured."""
        # Just check if we have an API key, the client can be created when needed
        return bool(self.api_key)

    def _call_api(self, messages: List[Dict]) -> str:
        """
        Call the AI API and return response text.

        v1.6.1: æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°ä» ConfigManager è¯»å–æœ€æ–°é…ç½®

        Args:
            messages: List of message dictionaries with 'role' and 'content'

        Returns:
            Response text from AI

        Raises:
            Exception: If API call fails
        """
        # v1.6.1: æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°åŠ è½½é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°è®¾ç½®
        self._reload_ai_settings()

        # Lazy initialization: create client when needed
        if not self.client:
            if not self.api_key:
                raise Exception("API Key not configured. Please set OPENAI_API_KEY in .env file")

            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.api_base
                )
                print(f"[AI Client] Lazy initialization successful")
            except Exception as e:
                raise Exception(f"Failed to initialize AI client: {str(e)}")

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            # Extract response text
            assistant_message = response.choices[0].message.content
            return assistant_message.strip()

        except Exception as e:
            raise Exception(f"API call failed: {str(e)}")

    def _call_api_stream(self, messages: List[Dict], chunk_signal: pyqtSignal, finished_signal: pyqtSignal):
        """
        æµå¼è°ƒç”¨ AI APIï¼Œé€å—å‘é€å“åº”

        v1.6.1: æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°ä» ConfigManager è¯»å–æœ€æ–°é…ç½®

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            chunk_signal: æ¯æ”¶åˆ°ä¸€å—å†…å®¹æ—¶å‘å‡ºçš„ä¿¡å·
            finished_signal: æµå¼è°ƒç”¨å®Œæˆæ—¶å‘å‡ºçš„ä¿¡å·

        Raises:
            Exception: If API call fails
        """
        # v1.6.1: æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°åŠ è½½é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°è®¾ç½®
        self._reload_ai_settings()

        # Lazy initialization: create client when needed
        if not self.client:
            if not self.api_key:
                raise Exception("API Key not configured. Please set OPENAI_API_KEY in .env file")

            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.api_base
                )
                print(f"[AI Client] Lazy initialization successful")
            except Exception as e:
                raise Exception(f"Failed to initialize AI client: {str(e)}")

        try:
            # æµå¼è°ƒç”¨
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            full_response = ""
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    # å‘é€æ–°å†…å®¹å—
                    chunk_signal.emit(content)

            # æµå¼è°ƒç”¨å®Œæˆ
            finished_signal.emit(full_response)

        except Exception as e:
            raise Exception(f"API streaming call failed: {str(e)}")

    def ask_async(self, user_message: str, terminal_context: str = ""):
        """
        Send question to AI asynchronously (non-blocking).
        é»˜è®¤ä½¿ç”¨æµå¼è°ƒç”¨ã€‚

        Args:
            user_message: User's question
            terminal_context: Recent terminal output for context
        """
        # é»˜è®¤ä½¿ç”¨æµå¼è°ƒç”¨
        return self.ask_async_stream(user_message, terminal_context)

    def ask_async_stream(self, user_message: str, terminal_context: str = ""):
        """
        æµå¼å¼‚æ­¥å‘é€é—®é¢˜åˆ° AIï¼Œå®æ—¶æ˜¾ç¤ºå“åº”ã€‚

        Args:
            user_message: User's question
            terminal_context: Recent terminal output for context
        """
        # Build messages
        messages = self._build_messages(user_message, terminal_context)

        # Add to conversation history
        self.conversation_history.append({"role": "user", "content": user_message})

        # å‘å‡ºæµå¼å¼€å§‹ä¿¡å·
        self.stream_started.emit()

        # Create worker thread with streaming enabled
        worker = AIResponseThread(self, messages, stream=True, parent=self)
        worker.stream_finished.connect(self._on_stream_finished)
        worker.error_occurred.connect(self._on_error)

        # è¿æ¥æµå¼å—ä¿¡å·åˆ° AIClient çš„æµå¼ä¿¡å·ï¼ˆé€ä¼ ç»™ UIï¼‰
        worker.stream_chunk_received.connect(self.stream_chunk_received.emit)

        worker.start()

    def _on_stream_finished(self, full_response: str):
        """æµå¼è°ƒç”¨å®Œæˆæ—¶çš„å¤„ç†ã€‚"""
        # Add to conversation history
        self.conversation_history.append({"role": "assistant", "content": full_response})

        # å‘å‡ºæµå¼å®Œæˆä¿¡å·
        self.stream_finished.emit(full_response)

    def ask_sync(self, user_message: str, terminal_context: str = "") -> str:
        """
        Send question to AI synchronously (blocking).

        Args:
            user_message: User's question
            terminal_context: Recent terminal output for context

        Returns:
            AI response text
        """
        messages = self._build_messages(user_message, terminal_context)
        self.conversation_history.append({"role": "user", "content": user_message})

        response = self._call_api(messages)

        # Add to conversation history
        self.conversation_history.append({"role": "assistant", "content": response})

        return response

    def _build_messages(self, user_message: str, terminal_context: str) -> List[Dict]:
        """
        Build message list for API call.

        Args:
            user_message: User's question
            terminal_context: Recent terminal output

        Returns:
            List of message dictionaries
        """
        messages = []

        # System prompt - ä½¿ç”¨é…ç½®çš„ç³»ç»Ÿæç¤ºè¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        system_prompt = getattr(self, 'system_prompt', self.DEFAULT_SYSTEM_PROMPT)
        messages.append({
            "role": "system",
            "content": system_prompt
        })

        # Add conversation history (excluding the last user message which will be added below)
        # This maintains context of the ongoing conversation
        # Keep only recent history to save tokens (configurable via max_history)
        if len(self.conversation_history) >= 2:
            # Use max_history from config (default 10 messages = 5 turns)
            history_limit = getattr(self, 'max_history', 10) * 2  # Convert to message count
            recent_history = self.conversation_history[-history_limit:] if len(self.conversation_history) > history_limit else self.conversation_history[:-1]
            messages.extend(recent_history)

        # Add terminal context if available
        if terminal_context:
            context_msg = f"å½“å‰ç»ˆç«¯å±å¹•å†…å®¹ï¼š\n```\n{terminal_context}\n```\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"
            messages.append({
                "role": "user",
                "content": context_msg
            })
        else:
            messages.append({
                "role": "user",
                "content": user_message
            })

        return messages

    def _on_response_received(self, response: str):
        """Handle response from worker thread."""
        # Add to conversation history
        self.conversation_history.append({"role": "assistant", "content": response})

        # Emit signal
        self.response_received.emit(response)

    def _on_error(self, error_msg: str):
        """Handle error from worker thread."""
        # Remove last user message from history since it failed
        if self.conversation_history and self.conversation_history[-1]["role"] == "user":
            self.conversation_history.pop()

        # Emit error signal
        self.error_occurred.emit(error_msg)

    def clear_history(self):
        """Clear conversation history."""
        self.conversation_history = []

    def set_config(self, api_key: str, api_base: str = None, model: str = None):
        """
        Update AI client configuration.

        Args:
            api_key: OpenAI API key
            api_base: API base URL (optional)
            model: Model name (optional)
        """
        self.api_key = api_key
        if api_base:
            self.api_base = api_base
        if model:
            self.model = model

        # Reinitialize client
        self._init_client()

    def set_profile(self, profile_name: str):
        """
        è®¾ç½®ä½¿ç”¨çš„ AI é…ç½®å¹¶é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯

        v1.6.1: å¤š AI API é…ç½®æ”¯æŒï¼Œä¿ç•™å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡

        Args:
            profile_name: AI é…ç½®åç§°
        """
        # ä¿å­˜ç°æœ‰çš„å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
        existing_history = self.conversation_history.copy()

        # åˆ‡æ¢é…ç½®
        self._profile_name = profile_name
        self._load_config(profile_name)
        self._init_client()

        # æ¢å¤å¯¹è¯å†å²ï¼Œä¿ç•™ä¸Šä¸‹æ–‡
        self.conversation_history = existing_history

        print(f"[DEBUG] AI Client switched to profile: {profile_name} (conversation history preserved: {len(existing_history)} messages)")
